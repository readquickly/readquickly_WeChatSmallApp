import re
import time
from Spider import *

class WeiboSpider(Spider):
    """
    çˆ¬å– [å¾®åšæ‰‹æœºWebç‰ˆ](https://m.weibo.cn/) ä¸Šçš„ [äººæ°‘æ—¥æŠ¥](https://m.weibo.cn/u/2803301701)
    é€šè¿‡ä¼ªé€  Ajax è¯·æ±‚ï¼Œçˆ¬å–ç›®æ ‡çš„å¾®åšé¡µã€‚
    Ajax è¯·æ±‚ä¸­éœ€è¦ä¸¤ä¸ªå…³é”®å€¼ï¼š
        * uid æˆ–å« oid -> å¯ä»¥åœ¨ https://weibo.com/ ä¸­å¯åœ¨ html ä¸­çš„ /html/head/script[4]/text() ä¸­çœ‹åˆ° ä¸€ä¸ª key ä¸º `oid` çš„å€¼ã€‚ï¼ˆæœ‰ä¸€ä¸ªæ³¨é‡Š<!-- $CONFIG -->ä¸‹é¢çš„è„šæœ¬ä¸­ï¼‰
        * containerid -> è¯·æ±‚ `https://m.weibo.cn/api/container/getIndex?type=uid&value={oid}` å¾—åˆ°çš„ json ä¸­ `this.data.tabsInfo.tabs[1].containerid` ï¼ˆ"title": "å¾®åš" è¿™ä¸€ç»„ä¸­çš„ containeridï¼‰
    """

    def __init__(self):
        self.oid = '2803301701'
        self.containerid = '1076032803301701'
        self.data_set({'source': 'äººæ°‘æ—¥æŠ¥|å¾®åš'})

        # æ„é€  Ajax è¯·æ±‚åˆ¶å®šç”¨æˆ· â€œå¾®åšâ€ é¡µé¢ ä½¿ç”¨çš„ url
        self.basicUrl = 'https://m.weibo.cn/api/container/getIndex?type=uid&value={oid}&containerid={containerid}'.format(
            oid=self.oid,
            containerid=self.containerid
            )

        # ä»¿åˆ¶è¯·æ±‚å¤´
        self.headers_set({
                'Accept': 'application/json, text/plain, */*',
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.0.3 Safari/605.1.15',
                'X-Requested-With': 'XMLHttpRequest',
                'MWeibo-Pwa': '1'
                })

    def getData(self, response):
        def parse_html(html):
            soup = BeautifulSoup(html, 'lxml')
            return soup.get_text()

        def get_weibo_pic(mblog):
            pic = None
            if mblog.get('original_pic'):           # å¸¦å›¾ç‰‡çš„å¾®åšï¼Œæä¾›æœ‰ä¸‰ç§è´¨é‡çš„å›¾ç‰‡ï¼šthumbnail_picï¼Œbmiddle_picï¼Œoriginal_pic
                pic = mblog.get('original_pic')
            elif mblog.get('page_info'):            # å¸¦è§†é¢‘çš„å¾®åš
                pic = mblog.get('page_info').get('page_pic').get('url')
            elif mblog.get('retweeted_status'):     # è½¬å‘&è¯„è®ºçš„å¾®åš
                pic = mblog.get('retweeted_status').get('bmiddle_pic')
            return pic or ''

        try:
            json = response.json()
            result = []

            for item in json.get('data').get('cards'):
                useful = self.data
                print('>>>', item.get('scheme'))
                useful['href'] = item.get('scheme')
                mblog = item.get('mblog')
                if not mblog:
                    continue
                else:
                    if mblog.get('text').find('ã€') != -1:
                        part = re.findall(
                            r'ã€(.*?)ã€‘(.*)',
                            mblog.get('text')
                            )[0]
                        useful['title'] = parse_html(part[0])
                        useful['text'] = parse_html(part[1])
                    else:
                        useful['title'] = ''
                        useful['text'] = parse_html(mblog.get('text'))
                    useful['time'] = mblog.get('created_at')
                    useful['pic'] = get_weibo_pic(mblog)
                print('###', useful)
                result.append(useful)           # BUG
                print('@@@@@@ğŸ‘‡\n', result, '@@@@@ğŸ‘†')
        except Exception as e:
            print('[getData Error]', e)
        finally:
            return result

    def saveData(self, res):
        for i in res:
            print(i['href'], i['time'])
        pass

    def run(self):
        try:
            for page in range(3):
                url = self.basicUrl + '&page=%s' % page
                print(url)
                page = self.getPage(url)
                res = self.getData(page)
                self.saveData(res)
                time.sleep(0.2)
            return True
        except Exception as e:
            print('[Spider.run Error] ', e)
            return False


if __name__ == '__main__':
    w = WeiboSpider()
    w.run()